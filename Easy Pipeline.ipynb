{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d0b4552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from tuw_nlp.text.pipeline import CachedStanzaPipeline, CustomStanzaPipeline\n",
    "from tuw_nlp.graph.utils import GraphMatcher\n",
    "from tuw_nlp.graph.utils import (\n",
    "    get_root_id,\n",
    "    graph_to_isi,\n",
    "    sen_to_graph\n",
    ")\n",
    "from stanza.models.common.doc import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721a3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": [\n",
    "        {\"sens\": [\n",
    "            {\"text\": \"This my favorite sentence.\"},\n",
    "            {\"text\": \"Yesterday, I had noodles.\"},\n",
    "            {\"text\": \"brown dog\"}]}],\n",
    "    \"cache_dir\": \".cache\",\n",
    "    \"memory\": \"8G\",\n",
    "    \"ALTO_JAR\": os.path.expanduser(\"~/tuw_nlp_resources/alto-2.3.6-SNAPSHOT-all.jar\")\n",
    "    \"grammar_fn\"\n",
    "    \"input_int\": \"ud\",\n",
    "    \"output_int\": \"fd\",\n",
    "    \"output_codec\": \"amr-sgraph-src\",\n",
    "    \"output_fn\"\n",
    "}\n",
    "\n",
    "sections = config[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd470fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 11:02:04 INFO: Loading these models for language: de (German):\n",
      "========================\n",
      "| Processor  | Package |\n",
      "------------------------\n",
      "| tokenize   | gsd     |\n",
      "| fix_ssplit | default |\n",
      "========================\n",
      "\n",
      "INFO:stanza:Loading these models for language: de (German):\n",
      "========================\n",
      "| Processor  | Package |\n",
      "------------------------\n",
      "| tokenize   | gsd     |\n",
      "| fix_ssplit | default |\n",
      "========================\n",
      "\n",
      "2021-11-08 11:02:04 INFO: Use device: gpu\n",
      "INFO:stanza:Use device: gpu\n",
      "2021-11-08 11:02:04 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2021-11-08 11:02:04 INFO: Loading: fix_ssplit\n",
      "INFO:stanza:Loading: fix_ssplit\n",
      "2021-11-08 11:02:04 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n",
      "2021-11-08 11:02:04 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "INFO:stanza:Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-11-08 11:02:04 INFO: Use device: gpu\n",
      "INFO:stanza:Use device: gpu\n",
      "2021-11-08 11:02:04 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2021-11-08 11:02:04 INFO: Loading: mwt\n",
      "INFO:stanza:Loading: mwt\n",
      "2021-11-08 11:02:04 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2021-11-08 11:02:04 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2021-11-08 11:02:04 INFO: Loading: depparse\n",
      "INFO:stanza:Loading: depparse\n",
      "2021-11-08 11:02:05 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n",
      "WARNING:root:loading NLP cache from .cache/nlp_cache.json...\n",
      "WARNING:root:done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sens': [{'text': 'This my favorite sentence.',\n",
       "    'tokens': [{'id': 1,\n",
       "      'text': 'This',\n",
       "      'lemma': 'This',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 0,\n",
       "      'deprel': 'root',\n",
       "      'misc': 'start_char=0|end_char=4'},\n",
       "     {'id': 2,\n",
       "      'text': 'my',\n",
       "      'lemma': 'my',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 4,\n",
       "      'deprel': 'nsubj',\n",
       "      'misc': 'start_char=5|end_char=7'},\n",
       "     {'id': 3,\n",
       "      'text': 'favorite',\n",
       "      'lemma': 'favorite',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 4,\n",
       "      'deprel': 'nsubj',\n",
       "      'misc': 'start_char=8|end_char=16'},\n",
       "     {'id': 4,\n",
       "      'text': 'sentence',\n",
       "      'lemma': 'sentence',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 1,\n",
       "      'deprel': 'flat',\n",
       "      'misc': 'start_char=17|end_char=25'},\n",
       "     {'id': 5,\n",
       "      'text': '.',\n",
       "      'lemma': '.',\n",
       "      'upos': 'PUNCT',\n",
       "      'xpos': '$.',\n",
       "      'head': 4,\n",
       "      'deprel': 'punct',\n",
       "      'misc': 'start_char=25|end_char=26'}]},\n",
       "   {'text': 'Yesterday, I had noodles.',\n",
       "    'tokens': [{'id': 1,\n",
       "      'text': 'Yesterday',\n",
       "      'lemma': 'Yesterday',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'NE',\n",
       "      'head': 0,\n",
       "      'deprel': 'root',\n",
       "      'misc': 'start_char=0|end_char=9'},\n",
       "     {'id': 2,\n",
       "      'text': ',',\n",
       "      'lemma': ',',\n",
       "      'upos': 'PUNCT',\n",
       "      'xpos': '$,',\n",
       "      'head': 3,\n",
       "      'deprel': 'punct',\n",
       "      'misc': 'start_char=9|end_char=10'},\n",
       "     {'id': 3,\n",
       "      'text': 'I',\n",
       "      'lemma': 'I',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 1,\n",
       "      'deprel': 'appos',\n",
       "      'misc': 'start_char=11|end_char=12'},\n",
       "     {'id': 4,\n",
       "      'text': 'had',\n",
       "      'lemma': 'had',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 5,\n",
       "      'deprel': 'nsubj',\n",
       "      'misc': 'start_char=13|end_char=16'},\n",
       "     {'id': 5,\n",
       "      'text': 'noodles',\n",
       "      'lemma': 'noodles',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 1,\n",
       "      'deprel': 'appos',\n",
       "      'misc': 'start_char=17|end_char=24'},\n",
       "     {'id': 6,\n",
       "      'text': '.',\n",
       "      'lemma': '.',\n",
       "      'upos': 'PUNCT',\n",
       "      'xpos': '$.',\n",
       "      'head': 1,\n",
       "      'deprel': 'punct',\n",
       "      'misc': 'start_char=24|end_char=25'}]},\n",
       "   {'text': 'brown dog',\n",
       "    'tokens': [{'id': 1,\n",
       "      'text': 'brown',\n",
       "      'lemma': 'brown',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'NE',\n",
       "      'feats': 'Case=Nom|Number=Sing',\n",
       "      'head': 0,\n",
       "      'deprel': 'root',\n",
       "      'misc': 'start_char=0|end_char=5'},\n",
       "     {'id': 2,\n",
       "      'text': 'dog',\n",
       "      'lemma': 'dog',\n",
       "      'upos': 'PUNCT',\n",
       "      'xpos': '$(',\n",
       "      'head': 1,\n",
       "      'deprel': 'punct',\n",
       "      'misc': 'start_char=6|end_char=9'}]}]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_pipeline = CustomStanzaPipeline(\n",
    "        processors='tokenize,mwt,pos,lemma,depparse')\n",
    "nlp_cache = os.path.join(config[\"cache_dir\"], 'nlp_cache.json')\n",
    "\n",
    "result = []\n",
    "\n",
    "with CachedStanzaPipeline(nlp_pipeline, nlp_cache) as nlp:\n",
    "    for section in sections:\n",
    "        for sen in section[\"sens\"]:\n",
    "            if \"tokens\" not in sen:\n",
    "                sen[\"tokens\"] = nlp(sen[\"text\"]).sentences[0].to_dict()\n",
    "                \n",
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2bed846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sens': [{'text': 'This my favorite sentence.',\n",
       "    'tokens': [{'id': (1,),\n",
       "      'text': 'This',\n",
       "      'lemma': 'This',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 0,\n",
       "      'deprel': 'root',\n",
       "      'misc': 'start_char=0|end_char=4'},\n",
       "     {'id': (2,),\n",
       "      'text': 'my',\n",
       "      'lemma': 'my',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 4,\n",
       "      'deprel': 'nsubj',\n",
       "      'misc': 'start_char=5|end_char=7'},\n",
       "     {'id': (3,),\n",
       "      'text': 'favorite',\n",
       "      'lemma': 'favorite',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 4,\n",
       "      'deprel': 'nsubj',\n",
       "      'misc': 'start_char=8|end_char=16'},\n",
       "     {'id': (4,),\n",
       "      'text': 'sentence',\n",
       "      'lemma': 'sentence',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 1,\n",
       "      'deprel': 'flat',\n",
       "      'misc': 'start_char=17|end_char=25'},\n",
       "     {'id': (5,),\n",
       "      'text': '.',\n",
       "      'lemma': '.',\n",
       "      'upos': 'PUNCT',\n",
       "      'xpos': '$.',\n",
       "      'head': 4,\n",
       "      'deprel': 'punct',\n",
       "      'misc': 'start_char=25|end_char=26'}],\n",
       "    'isi': 'ROOT(PROPN(_FLAT(PROPN(_NSUBJ(PROPN(my)), PROPN(_NSUBJ(PROPN(favorite)), PROPN(_PUNCT(PUNCT(PERIOD)), PROPN(sentence))))), PROPN(This)))',\n",
       "    'graph': <networkx.classes.digraph.DiGraph at 0x7f44b1428af0>},\n",
       "   {'text': 'Yesterday, I had noodles.',\n",
       "    'tokens': [{'id': (1,),\n",
       "      'text': 'Yesterday',\n",
       "      'lemma': 'Yesterday',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'NE',\n",
       "      'head': 0,\n",
       "      'deprel': 'root',\n",
       "      'misc': 'start_char=0|end_char=9'},\n",
       "     {'id': (2,),\n",
       "      'text': ',',\n",
       "      'lemma': ',',\n",
       "      'upos': 'PUNCT',\n",
       "      'xpos': '$,',\n",
       "      'head': 3,\n",
       "      'deprel': 'punct',\n",
       "      'misc': 'start_char=9|end_char=10'},\n",
       "     {'id': (3,),\n",
       "      'text': 'I',\n",
       "      'lemma': 'I',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 1,\n",
       "      'deprel': 'appos',\n",
       "      'misc': 'start_char=11|end_char=12'},\n",
       "     {'id': (4,),\n",
       "      'text': 'had',\n",
       "      'lemma': 'had',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 5,\n",
       "      'deprel': 'nsubj',\n",
       "      'misc': 'start_char=13|end_char=16'},\n",
       "     {'id': (5,),\n",
       "      'text': 'noodles',\n",
       "      'lemma': 'noodles',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'FM',\n",
       "      'feats': 'Foreign=Yes',\n",
       "      'head': 1,\n",
       "      'deprel': 'appos',\n",
       "      'misc': 'start_char=17|end_char=24'},\n",
       "     {'id': (6,),\n",
       "      'text': '.',\n",
       "      'lemma': '.',\n",
       "      'upos': 'PUNCT',\n",
       "      'xpos': '$.',\n",
       "      'head': 1,\n",
       "      'deprel': 'punct',\n",
       "      'misc': 'start_char=24|end_char=25'}],\n",
       "    'isi': 'ROOT(PROPN(_APPOS(PROPN(_PUNCT(PUNCT(COMMA)), PROPN(I))), PROPN(_APPOS(PROPN(_NSUBJ(PROPN(had)), PROPN(noodles))), PROPN(_PUNCT(PUNCT(PERIOD)), PROPN(Yesterday)))))',\n",
       "    'graph': <networkx.classes.digraph.DiGraph at 0x7f44df1019d0>},\n",
       "   {'text': 'brown dog',\n",
       "    'tokens': [{'id': (1,),\n",
       "      'text': 'brown',\n",
       "      'lemma': 'brown',\n",
       "      'upos': 'PROPN',\n",
       "      'xpos': 'NE',\n",
       "      'feats': 'Case=Nom|Number=Sing',\n",
       "      'head': 0,\n",
       "      'deprel': 'root',\n",
       "      'misc': 'start_char=0|end_char=5'},\n",
       "     {'id': (2,),\n",
       "      'text': 'dog',\n",
       "      'lemma': 'dog',\n",
       "      'upos': 'PUNCT',\n",
       "      'xpos': '$(',\n",
       "      'head': 1,\n",
       "      'deprel': 'punct',\n",
       "      'misc': 'start_char=6|end_char=9'}],\n",
       "    'isi': 'ROOT(PROPN(_PUNCT(PUNCT(dog)), PROPN(brown)))',\n",
       "    'graph': <networkx.classes.digraph.DiGraph at 0x7f44df0936a0>}]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for section in sections:\n",
    "    for sen in section[\"sens\"]:\n",
    "        parsed_sen = Document([sen[\"tokens\"]]).sentences[0]\n",
    "        graph = sen_to_graph(parsed_sen)\n",
    "        isi = graph_to_isi(graph)\n",
    "        sen[\"isi\"] = isi\n",
    "        sen[\"graph\"] = graph\n",
    "        \n",
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6192a2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (3239587295.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_23833/3239587295.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    '-o', output_fn]\u001b[0m\n\u001b[0m                    \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"command = ['java', f'-Xmx{config.memory}',\n",
    "           '-cp', config.ALTO_JAR, 'de.up.ling.irtg.script.ParsingEvaluator',\n",
    "           input_fn,\n",
    "           '-g', grammar_fn,\n",
    "           '-I', input_int,\n",
    "           '-O', f\"{config.output_int}={config.output_codec}\",\n",
    "           '-o', output_fn]\n",
    "           \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79b757b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 11:36:53 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "WARNING:stanza:Can not find mwt: default from official model list. Ignoring it.\n",
      "2021-11-08 11:36:53 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "| depparse  | ewt     |\n",
      "=======================\n",
      "\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "| depparse  | ewt     |\n",
      "=======================\n",
      "\n",
      "2021-11-08 11:36:53 INFO: Use device: gpu\n",
      "INFO:stanza:Use device: gpu\n",
      "2021-11-08 11:36:53 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2021-11-08 11:36:53 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2021-11-08 11:36:54 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2021-11-08 11:36:54 INFO: Loading: depparse\n",
      "INFO:stanza:Loading: depparse\n",
      "2021-11-08 11:36:54 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n",
      "WARNING:root:creating new NLP cache in en_nlp_cache\n",
      "WARNING:root:setting up new cache file: cache/UD_FL.json\n",
      "Processing tmp/20211108_113706_250090/input.txt (2 instances) ...\n",
      "1 [ROOT(NOUN(_AMOD(ADJ(brown)),NOUN(dog)))           ] 35 ms\n",
      "2 [dummy(dummy)                                      ] 272 μs\n",
      "Done, total time: 55 ms\n",
      "WARNING:root:loaded cache from cache/UD_FL.json with interpretations: ['fl', 'ud']\n",
      "WARNING:root:updated cache in cache/UD_FL.json\n",
      "Processing tmp/20211108_113707_135291/input.txt (2 instances) ...\n",
      "1 [ROOT(NOUN(_DET(DET(a)),NOUN(_PUNCT(PUNCT(COMMA)),N] 50 ms\n",
      "2 [dummy(dummy)                                      ] 485 μs\n",
      "Done, total time: 72 ms\n",
      "WARNING:root:loaded cache from cache/UD_FL.json with interpretations: ['fl', 'ud']\n",
      "WARNING:root:updated cache in cache/UD_FL.json\n",
      "Processing tmp/20211108_113707_616550/input.txt (2 instances) ...\n",
      "1 [ROOT(NOUN(_DET(DET(a)),NOUN(_NMOD(PRON(_CASE(ADP(l] 35 ms\n",
      "2 [dummy(dummy)                                      ] 284 μs\n",
      "Done, total time: 51 ms\n",
      "WARNING:root:loaded cache from cache/UD_FL.json with interpretations: ['fl', 'ud']\n",
      "WARNING:root:updated cache in cache/UD_FL.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NodeDataView({0: {'name': 'dog', 'expanded': True}, 1: {'name': 'brown', 'expanded': True}, 2: {'name': 'mammal'}, 3: {'name': 'familiaris'}, 4: {'name': 'domesticate'}, 5: {'name': 'of'}, 6: {'name': 'appearance'}, 7: {'name': 'variable'}, 8: {'name': 'highly'}, 9: {'name': 'due'}, 10: {'name': 'breeding'}, 11: {'name': 'human'}, 12: {'name': 'for'}, 13: {'name': 'thousand'}, 14: {'name': 'year'}, 15: {'name': 'lupus'}, 16: {'name': 'canis'}, 17: {'name': 'colour'}, 18: {'name': 'like'}, 19: {'name': 'that'}, 20: {'name': 'COORD'}, 21: {'name': 'coffee'}, 22: {'name': 'chocolate'}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tuw_nlp.grammar.text_to_4lang import TextTo4lang\n",
    "\n",
    "tfl = TextTo4lang(\"en\", \"en_nlp_cache\")\n",
    "\n",
    "fl_graphs = list(tfl(\"brown dog\", depth=1, substitute=False))\n",
    "\n",
    "# Then the fl_graphs will directly contain a networkx graph object\n",
    "fl_graphs[0].nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19eccb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutEdgeDataView([(0, 1, {'color': 0}), (0, 2, {'color': 0}), (1, 17, {'color': 0}), (2, 3, {'color': 0}), (3, 4, {'color': 0}), (3, 15, {'color': 0}), (3, 16, {'color': 0}), (4, 3, {'color': 1}), (5, 4, {'color': 1}), (5, 6, {'color': 2}), (5, 13, {'color': 1}), (5, 14, {'color': 2}), (5, 19, {'color': 1}), (5, 20, {'color': 2}), (6, 7, {'color': 0}), (7, 8, {'color': 0}), (9, 6, {'color': 1}), (9, 10, {'color': 2}), (10, 11, {'color': 0}), (12, 4, {'color': 1}), (12, 13, {'color': 2}), (18, 17, {'color': 1}), (18, 19, {'color': 2}), (20, 21, {'color': 0}), (20, 22, {'color': 0})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_graphs[0].edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084407e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
